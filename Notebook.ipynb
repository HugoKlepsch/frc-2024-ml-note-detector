{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f7163c-896a-4f88-95a3-a1073e22b6c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T19:22:25.998273822Z",
     "start_time": "2024-01-27T19:22:25.917114909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf56d7ed-a0ab-4cee-a8a6-7a50e9731a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\n",
      "Setup complete âœ… (8 CPUs, 31.1 GB RAM, 36.0/48.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758831b9-62c5-42f0-bf38-b83f52642ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-27T19:22:32.799185687Z",
     "start_time": "2024-01-27T19:22:29.030174214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\r\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\r\n",
      "\r\n",
      "Found https://ultralytics.com/images/zidane.jpg locally at zidane.jpg\r\n",
      "image 1/1 /home/hugo/git/robotics/2024/2024-note/zidane.jpg: 384x640 2 persons, 1 tie, 71.9ms\r\n",
      "Speed: 3.2ms preprocess, 71.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\r\n",
      "Results saved to \u001B[1m/home/hugo/git/robotics/2024/2024-note/runs/detect/predict3\u001B[0m\r\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\r\n"
     ]
    }
   ],
   "source": [
    "# Run inference on an image with YOLOv8n\n",
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780M/780M [01:24<00:00, 9.74MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace datasets/coco/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
    "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:24:55.920967963Z",
     "start_time": "2024-01-27T19:22:32.802620041Z"
    }
   },
   "id": "7d32e8191bb2708f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\r\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/hugo/git/robotics/2024/datasets/coco8/labels/val.cache... 4 \u001B[0m\r\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\r\n",
      "                   all          4         17      0.621      0.833      0.888       0.63\r\n",
      "                person          4         10      0.721        0.5      0.519      0.269\r\n",
      "                   dog          4          1       0.37          1      0.995      0.597\r\n",
      "                 horse          4          2      0.751          1      0.995      0.631\r\n",
      "              elephant          4          2      0.505        0.5      0.828      0.394\r\n",
      "              umbrella          4          1      0.564          1      0.995      0.995\r\n",
      "          potted plant          4          1      0.814          1      0.995      0.895\r\n",
      "Speed: 0.9ms preprocess, 49.7ms inference, 0.0ms loss, 0.7ms postprocess per image\r\n",
      "Results saved to \u001B[1m/home/hugo/git/robotics/2024/2024-note/runs/detect/val2\u001B[0m\r\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\r\n"
     ]
    }
   ],
   "source": [
    "# Validate YOLOv8n on COCO8 val\n",
    "!yolo val model=yolov8n.pt data=coco8.yaml"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:25:02.841568863Z",
     "start_time": "2024-01-27T19:24:55.958437683Z"
    }
   },
   "id": "3713295888b2f042",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\r\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/hugo/git/robotics/2024/2024-note/runs/detect/train2\r\n",
      "\r\n",
      "                   from  n    params  module                                       arguments                     \r\n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \r\n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \r\n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \r\n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \r\n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \r\n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \r\n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \r\n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \r\n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \r\n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \r\n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \r\n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \r\n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \r\n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \r\n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \r\n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \r\n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \r\n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\r\n",
      "\r\n",
      "Transferred 355/355 items from pretrained weights\r\n",
      "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir /home/hugo/git/robotics/2024/2024-note/runs/detect/train2', view at http://localhost:6006/\r\n",
      "Freezing layer 'model.22.dfl.conv.weight'\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/hugo/git/robotics/2024/datasets/coco8/labels/train.cache..\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/hugo/git/robotics/2024/datasets/coco8/labels/val.cache... 4 \u001B[0m\r\n",
      "Plotting labels to /home/hugo/git/robotics/2024/2024-note/runs/detect/train2/labels.jpg... \r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mmodel graph visualization added âœ…\r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 0 dataloader workers\r\n",
      "Logging results to \u001B[1m/home/hugo/git/robotics/2024/2024-note/runs/detect/train2\u001B[0m\r\n",
      "Starting training for 3 epochs...\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "        1/3         0G     0.9795      3.465      1.357         32        640: 1\r\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\r\n",
      "                   all          4         17      0.902      0.518      0.727      0.494\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "        2/3         0G      1.362      2.845      1.662         22        640: 1\r\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\r\n",
      "                   all          4         17      0.906      0.524      0.738      0.515\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "        3/3         0G      1.294      3.584      1.698         17        640: 1\r\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\r\n",
      "                   all          4         17      0.907      0.525      0.729       0.52\r\n",
      "\r\n",
      "3 epochs completed in 0.001 hours.\r\n",
      "Optimizer stripped from /home/hugo/git/robotics/2024/2024-note/runs/detect/train2/weights/last.pt, 6.5MB\r\n",
      "Optimizer stripped from /home/hugo/git/robotics/2024/2024-note/runs/detect/train2/weights/best.pt, 6.5MB\r\n",
      "\r\n",
      "Validating /home/hugo/git/robotics/2024/2024-note/runs/detect/train2/weights/best.pt...\r\n",
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\r\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\r\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\r\n",
      "                   all          4         17      0.907      0.525      0.743      0.527\r\n",
      "                person          4         10       0.94        0.3      0.502      0.236\r\n",
      "                   dog          4          1          1          0      0.332      0.189\r\n",
      "                 horse          4          2          1      0.852      0.995      0.747\r\n",
      "              elephant          4          2          1          0      0.638        0.2\r\n",
      "              umbrella          4          1      0.759          1      0.995      0.895\r\n",
      "          potted plant          4          1      0.745          1      0.995      0.895\r\n",
      "Speed: 0.7ms preprocess, 48.0ms inference, 0.0ms loss, 1.0ms postprocess per image\r\n",
      "Results saved to \u001B[1m/home/hugo/git/robotics/2024/2024-note/runs/detect/train2\u001B[0m\r\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\r\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv8n on COCO8 for 3 epochs\n",
    "!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:25:20.688918725Z",
     "start_time": "2024-01-27T19:25:02.860258313Z"
    }
   },
   "id": "7bc69c418791366c",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Export\n",
    "\n",
    "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
    "\n",
    "- ðŸ’¡ ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.\n",
    "- ðŸ’¡ ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n",
    "\n",
    "\n",
    "| Format                                                             | `format` Argument | Model                     | Metadata | Arguments                                           |\n",
    "|--------------------------------------------------------------------|-------------------|---------------------------|----------|-----------------------------------------------------|\n",
    "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | âœ…        | -                                                   |\n",
    "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | âœ…        | `imgsz`, `optimize`                                 |\n",
    "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |\n",
    "| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | âœ…        | `imgsz`, `half`, `int8`                             |\n",
    "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |\n",
    "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlpackage`       | âœ…        | `imgsz`, `half`, `int8`, `nms`                      |\n",
    "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | âœ…        | `imgsz`, `keras`, `int8`                            |\n",
    "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | âŒ        | `imgsz`                                             |\n",
    "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | âœ…        | `imgsz`, `half`, `int8`                             |\n",
    "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | âœ…        | `imgsz`                                             |\n",
    "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | âœ…        | `imgsz`                                             |\n",
    "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | âœ…        | `imgsz`                                             |\n",
    "| [ncnn](https://github.com/Tencent/ncnn)                            | `ncnn`            | `yolov8n_ncnn_model/`     | âœ…        | `imgsz`, `half`                                     |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0107b0725fbc5c3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.6 torch-2.1.2+cu121 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)\r\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mPyTorch:\u001B[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mTensorFlow SavedModel:\u001B[0m starting export with tensorflow 2.15.0...\r\n",
      "WARNING âš ï¸ tensorflow<=2.13.1 is required, but tensorflow==2.15.0 is currently installed https://github.com/ultralytics/ultralytics/issues/5161\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m starting export with onnx 1.15.0 opset 17...\r\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m simplifying with onnxsim 0.4.35...\r\n",
      "\u001B[34m\u001B[1mONNX:\u001B[0m export success âœ… 1.1s, saved as 'yolov8n.onnx' (12.3 MB)\r\n",
      "\u001B[34m\u001B[1mTensorFlow SavedModel:\u001B[0m running 'onnx2tf -i \"yolov8n.onnx\" -o \"yolov8n_saved_model\" -nuo --non_verbose'\r\n",
      "Summary on the non-converted ops:\r\n",
      "---------------------------------\r\n",
      " * Accepted dialects: tfl, builtin, func\r\n",
      " * Non-Converted Ops: 155, Total Ops 410, % non-converted = 37.80 %\r\n",
      " * 155 ARITH ops\r\n",
      "\r\n",
      "- arith.constant:  155 occurrences  (f32: 131, i32: 24)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  (f32: 8)\r\n",
      "  (f32: 18)\r\n",
      "  (f32: 64)\r\n",
      "  (f32: 58)\r\n",
      "  (f32: 3)\r\n",
      "  (f32: 59)\r\n",
      "  (f32: 7)\r\n",
      "  (f32: 6)\r\n",
      "  (f32: 2)\r\n",
      "  (f32: 1)\r\n",
      "  (f32: 20)\r\n",
      "  (f32: 2)\r\n",
      "  (f32: 4)\r\n",
      "Summary on the non-converted ops:\r\n",
      "---------------------------------\r\n",
      " * Accepted dialects: tfl, builtin, func\r\n",
      " * Non-Converted Ops: 155, Total Ops 541, % non-converted = 28.65 %\r\n",
      " * 155 ARITH ops\r\n",
      "\r\n",
      "- arith.constant:  155 occurrences  (f16: 131, i32: 24)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "  (f32: 8)\r\n",
      "  (f32: 18)\r\n",
      "  (f32: 64)\r\n",
      "  (f32: 131)\r\n",
      "  (f32: 58)\r\n",
      "  (f32: 3)\r\n",
      "  (f32: 59)\r\n",
      "  (f32: 7)\r\n",
      "  (f32: 6)\r\n",
      "  (f32: 2)\r\n",
      "  (f32: 1)\r\n",
      "  (f32: 20)\r\n",
      "  (f32: 2)\r\n",
      "  (f32: 4)\r\n",
      "\u001B[34m\u001B[1mTensorFlow SavedModel:\u001B[0m export failure âŒ 19.3s: generic_type: cannot initialize type \"StatusCode\": an object with that name is already defined\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/bin/yolo\", line 8, in <module>\r\n",
      "    sys.exit(entrypoint())\r\n",
      "             ^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 568, in entrypoint\r\n",
      "    getattr(model, mode)(**overrides)  # default args from model\r\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 348, in export\r\n",
      "    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n",
      "    return func(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/exporter.py\", line 284, in __call__\r\n",
      "    f[5], keras_model = self.export_saved_model()\r\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/exporter.py\", line 136, in outer_func\r\n",
      "    raise e\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/exporter.py\", line 131, in outer_func\r\n",
      "    f, model = inner_func(*args, **kwargs)\r\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/exporter.py\", line 783, in export_saved_model\r\n",
      "    f.unlink() if \"quant_with_int16_act.tflite\" in str(f) else self._add_tflite_metadata(file)\r\n",
      "                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/ultralytics/engine/exporter.py\", line 894, in _add_tflite_metadata\r\n",
      "    from tflite_support import flatbuffers  # noqa\r\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/tflite_support/__init__.py\", line 53, in <module>\r\n",
      "    from tflite_support import task\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/tflite_support/task/__init__.py\", line 32, in <module>\r\n",
      "    from . import vision\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/tflite_support/task/vision/__init__.py\", line 20, in <module>\r\n",
      "    from tensorflow_lite_support.python.task.vision import image_classifier\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/tensorflow_lite_support/python/task/vision/image_classifier.py\", line 23, in <module>\r\n",
      "    from tensorflow_lite_support.python.task.vision.core import tensor_image\r\n",
      "  File \"/home/hugo/git/robotics/2024/2024-note/venv/lib/python3.11/site-packages/tensorflow_lite_support/python/task/vision/core/tensor_image.py\", line 19, in <module>\r\n",
      "    from tensorflow_lite_support.python.task.vision.core.pybinds import image_utils\r\n",
      "ImportError: generic_type: cannot initialize type \"StatusCode\": an object with that name is already defined\r\n"
     ]
    }
   ],
   "source": [
    "!yolo export model=yolov8n.pt format=tflite"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:25:44.409112927Z",
     "start_time": "2024-01-27T19:25:20.693399282Z"
    }
   },
   "id": "4fedc749c81e9d9d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:25:44.416067911Z",
     "start_time": "2024-01-27T19:25:44.410961357Z"
    }
   },
   "id": "f32e09c4b2b94276",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
